{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846126e0",
   "metadata": {},
   "source": [
    "# function to make text + image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class PDEImageTextDataset(Dataset):\n",
    "    def __init__(self, jsonl_path):\n",
    "        self.jsonl_path = Path(jsonl_path)\n",
    "        self.root = self.jsonl_path.parent  # ← IMPORTANT\n",
    "\n",
    "        with open(self.jsonl_path, \"r\") as f:\n",
    "            self.samples = [json.loads(l) for l in f]\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        image_path = self.root / sample[\"image\"]  # ← FIX\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        text = sample[\"text\"]\n",
    "\n",
    "        return self.transform(image), text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cdd2d5",
   "metadata": {},
   "source": [
    "# Vision Only CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "class VisionOnlyDataset(Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base = base_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.base[idx]\n",
    "        return image\n",
    "\n",
    "\n",
    "class VisionOnlyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "        self.backbone.fc = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "def train_vision_only(model, dataloader, optimizer, device, epochs=20):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            targets = torch.zeros(images.size(0), 1, device=device)\n",
    "\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.1344\n",
      "Epoch 2 | Loss 0.0014\n",
      "Epoch 3 | Loss 0.0003\n",
      "Epoch 4 | Loss 0.0001\n",
      "Epoch 5 | Loss 0.0001\n",
      "Epoch 6 | Loss 0.0001\n",
      "Epoch 7 | Loss 0.0001\n",
      "Epoch 8 | Loss 0.0000\n",
      "Epoch 9 | Loss 0.0000\n",
      "Epoch 10 | Loss 0.0000\n",
      "Epoch 11 | Loss 0.0000\n",
      "Epoch 12 | Loss 0.0000\n",
      "Epoch 13 | Loss 0.0000\n",
      "Epoch 14 | Loss 0.0000\n",
      "Epoch 15 | Loss 0.0000\n",
      "Epoch 16 | Loss 0.0000\n",
      "Epoch 17 | Loss 0.0000\n",
      "Epoch 18 | Loss 0.0000\n",
      "Epoch 19 | Loss 0.0000\n",
      "Epoch 20 | Loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "base_dataset = PDEImageTextDataset(\n",
    "    \"/Users/divyam/Course/Project Arbeit/pde_solver/vl_dataset/annotations.jsonl\"\n",
    ")\n",
    "\n",
    "vision_dataset = VisionOnlyDataset(base_dataset)\n",
    "vision_loader = DataLoader(vision_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "vision_model = VisionOnlyModel().to(device)\n",
    "optimizer = torch.optim.AdamW(vision_model.parameters(), lr=3e-4)\n",
    "\n",
    "train_vision_only(\n",
    "    vision_model,\n",
    "    vision_loader,\n",
    "    optimizer,\n",
    "    device=device,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6964d0a",
   "metadata": {},
   "source": [
    "\n",
    "# Vision Text CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VisionTextCLIP(nn.Module):\n",
    "    def __init__(self, vision_model, text_dim, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.vision = vision_model\n",
    "        self.vision.fc = nn.Identity()\n",
    "        self.image_proj = nn.Linear(512, embed_dim)\n",
    "\n",
    "        self.text_proj = nn.Linear(text_dim, embed_dim)\n",
    "\n",
    "    def forward(self, images, text_emb):\n",
    "        img_feat = self.image_proj(self.vision(images))\n",
    "        txt_feat = self.text_proj(text_emb)\n",
    "\n",
    "        img_feat = F.normalize(img_feat, dim=1)\n",
    "        txt_feat = F.normalize(txt_feat, dim=1)\n",
    "\n",
    "        return img_feat, txt_feat\n",
    "    \n",
    "def clip_loss(img_emb, txt_emb, temperature=0.07):\n",
    "    logits = img_emb @ txt_emb.T / temperature\n",
    "    labels = torch.arange(len(img_emb)).to(img_emb.device)\n",
    "\n",
    "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4d8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_clip(\n",
    "    model,\n",
    "    dataloader,\n",
    "    text_encoder,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=10\n",
    "):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, texts in dataloader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_emb = torch.from_numpy(\n",
    "                text_encoder.encode(texts)\n",
    "                ).to(device)\n",
    "\n",
    "            img_emb, txt_emb = model(images, text_emb)\n",
    "            loss = clip_loss(img_emb, txt_emb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7203b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCLIP(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            *list(backbone.children())[:-1]\n",
    "        )\n",
    "\n",
    "        self.image_proj = nn.Linear(512, embed_dim)\n",
    "        self.text_proj = nn.Linear(384, embed_dim)  # adjust to your text encoder\n",
    "\n",
    "    def forward(self, images, text_emb):\n",
    "        img_feat = self.image_encoder(images).squeeze(-1).squeeze(-1)\n",
    "        img_emb = F.normalize(self.image_proj(img_feat), dim=-1)\n",
    "        txt_emb = F.normalize(self.text_proj(text_emb), dim=-1)\n",
    "        return img_emb, txt_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99901e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "text_encoder = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 3.4788\n",
      "Epoch 2 | Loss 3.3713\n",
      "Epoch 3 | Loss 3.2135\n",
      "Epoch 4 | Loss 3.0028\n",
      "Epoch 5 | Loss 2.5931\n",
      "Epoch 6 | Loss 2.0965\n",
      "Epoch 7 | Loss 1.7331\n",
      "Epoch 8 | Loss 1.2601\n",
      "Epoch 9 | Loss 1.0609\n",
      "Epoch 10 | Loss 0.9560\n",
      "Epoch 11 | Loss 0.8178\n",
      "Epoch 12 | Loss 0.7361\n",
      "Epoch 13 | Loss 0.7432\n",
      "Epoch 14 | Loss 0.6881\n",
      "Epoch 15 | Loss 0.6874\n",
      "Epoch 16 | Loss 0.6179\n",
      "Epoch 17 | Loss 0.6154\n",
      "Epoch 18 | Loss 0.6362\n",
      "Epoch 19 | Loss 0.6069\n",
      "Epoch 20 | Loss 0.6248\n"
     ]
    }
   ],
   "source": [
    "# vision + correct text\n",
    "\n",
    "dataset = PDEImageTextDataset(\"/Users/divyam/Course/Project Arbeit/pde_solver/vl_dataset/annotations.jsonl\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SimpleCLIP(embed_dim=512).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "train_clip(\n",
    "    model,\n",
    "    loader,\n",
    "    text_encoder,\n",
    "    optimizer,\n",
    "    device=\"mps\",\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc694fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 3.4668\n",
      "Epoch 2 | Loss 3.3695\n",
      "Epoch 3 | Loss 3.3011\n",
      "Epoch 4 | Loss 3.0453\n",
      "Epoch 5 | Loss 2.6769\n",
      "Epoch 6 | Loss 2.1905\n",
      "Epoch 7 | Loss 1.6729\n",
      "Epoch 8 | Loss 1.4171\n",
      "Epoch 9 | Loss 1.0982\n",
      "Epoch 10 | Loss 0.9219\n",
      "Epoch 11 | Loss 0.7607\n",
      "Epoch 12 | Loss 0.7543\n",
      "Epoch 13 | Loss 0.7205\n",
      "Epoch 14 | Loss 0.6921\n",
      "Epoch 15 | Loss 0.6217\n",
      "Epoch 16 | Loss 0.6441\n",
      "Epoch 17 | Loss 0.6111\n",
      "Epoch 18 | Loss 0.5942\n",
      "Epoch 19 | Loss 0.5734\n",
      "Epoch 20 | Loss 0.6029\n"
     ]
    }
   ],
   "source": [
    "# vision + correct text\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "vision = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "vision.fc = nn.Linear(512, 1)  # regression\n",
    "\n",
    "model = vision\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "model = VisionTextCLIP(\n",
    "    vision_model=model,\n",
    "    text_dim=384,      \n",
    "    embed_dim=512\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "train_clip(\n",
    "    model,\n",
    "    loader,\n",
    "    text_encoder,\n",
    "    optimizer,\n",
    "    device=\"mps\",\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb13cc",
   "metadata": {},
   "source": [
    "# Vision + Shuffled text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9980da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ShuffledTextDataset(PDEImageTextDataset):\n",
    "    def __init__(self, jsonl_path):\n",
    "        super().__init__(jsonl_path)\n",
    "        self.texts = [s[\"text\"] for s in self.samples]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = super().__getitem__(idx)\n",
    "        random_text = random.choice(self.texts)\n",
    "        return image, random_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb0668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 4.2607\n",
      "Epoch 2 | Loss 3.4420\n",
      "Epoch 3 | Loss 3.4373\n",
      "Epoch 4 | Loss 3.4344\n",
      "Epoch 5 | Loss 3.4357\n",
      "Epoch 6 | Loss 3.4329\n",
      "Epoch 7 | Loss 3.4372\n",
      "Epoch 8 | Loss 3.4346\n",
      "Epoch 9 | Loss 3.4327\n",
      "Epoch 10 | Loss 3.4369\n",
      "Epoch 11 | Loss 3.4340\n",
      "Epoch 12 | Loss 3.4345\n",
      "Epoch 13 | Loss 3.4349\n",
      "Epoch 14 | Loss 3.4345\n",
      "Epoch 15 | Loss 3.4355\n",
      "Epoch 16 | Loss 3.4371\n",
      "Epoch 17 | Loss 3.4334\n",
      "Epoch 18 | Loss 3.4377\n",
      "Epoch 19 | Loss 3.4350\n",
      "Epoch 20 | Loss 3.4378\n"
     ]
    }
   ],
   "source": [
    "dataset = ShuffledTextDataset(\"/Users/divyam/Course/Project Arbeit/pde_solver/vl_dataset/annotations.jsonl\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_clip(\n",
    "    model,\n",
    "    loader,\n",
    "    text_encoder,\n",
    "    optimizer,\n",
    "    device=\"mps\",\n",
    "    epochs=20\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
