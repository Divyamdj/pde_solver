{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846126e0",
   "metadata": {},
   "source": [
    "# function to make text + image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c0fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PDETensorTextDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, dtype=torch.float32):\n",
    "        self.jsonl_path = Path(jsonl_path)\n",
    "        self.root = self.jsonl_path.parent\n",
    "        self.dtype = dtype\n",
    "\n",
    "        with open(self.jsonl_path, \"r\") as f:\n",
    "            self.samples = [json.loads(l) for l in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        tensor_path = self.root / sample[\"tensor\"]\n",
    "        tensor = np.load(tensor_path)\n",
    "\n",
    "        tensor = torch.from_numpy(tensor).to(self.dtype)\n",
    "        text = sample[\"text\"]\n",
    "\n",
    "        return tensor, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7f3d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 384]) torch.float32\n",
      "This model captures interactions between kinetic energy, pressure, and radiative cooling.\n"
     ]
    }
   ],
   "source": [
    "dataset = PDETensorTextDataset(\"/Users/divyam/Course/Project Arbeit/pde_solver/vl_dataset/annotations.jsonl\")\n",
    "\n",
    "sol, txt = dataset[0]\n",
    "print(sol.shape, sol.dtype)\n",
    "print(txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cdd2d5",
   "metadata": {},
   "source": [
    "# Vision Only CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed96ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class VisionOnlyPDEDataset(Dataset):\n",
    "    def __init__(self, base_dataset, input_steps=5, output_steps=10):\n",
    "        self.base = base_dataset\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        solution, _ = self.base[idx]   # [T, X]\n",
    "\n",
    "        x = solution[:self.input_steps]                         # [5, X]\n",
    "        y = solution[self.input_steps:self.input_steps+10]      # [10, X]\n",
    "\n",
    "        # Add spatial height dimension for Conv2d\n",
    "        x = x.unsqueeze(1)   # [5, 1, X]\n",
    "        y = y.unsqueeze(1)   # [10, 1, X]\n",
    "\n",
    "        return x.float(), y.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8132faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CNNPDEBaseline(nn.Module):\n",
    "    def __init__(self, input_steps, output_steps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_steps, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Conv2d(\n",
    "            128, output_steps, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, input_steps, 1, X]\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)   # [B, output_steps, 1, X]\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73093bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pde_baseline(model, dataloader, optimizer, device, epochs=20):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss {total_loss/len(dataloader):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1338c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 1253.299639\n",
      "Epoch 2 | Loss 127.280353\n",
      "Epoch 3 | Loss 103.441058\n",
      "Epoch 4 | Loss 98.048912\n",
      "Epoch 5 | Loss 94.347860\n",
      "Epoch 6 | Loss 91.586255\n",
      "Epoch 7 | Loss 88.877847\n",
      "Epoch 8 | Loss 87.742303\n",
      "Epoch 9 | Loss 86.730272\n",
      "Epoch 10 | Loss 86.029710\n",
      "Epoch 11 | Loss 84.509027\n",
      "Epoch 12 | Loss 83.746571\n",
      "Epoch 13 | Loss 83.422834\n",
      "Epoch 14 | Loss 82.732568\n",
      "Epoch 15 | Loss 82.440064\n",
      "Epoch 16 | Loss 81.879456\n",
      "Epoch 17 | Loss 81.127313\n",
      "Epoch 18 | Loss 81.156323\n",
      "Epoch 19 | Loss 80.740944\n",
      "Epoch 20 | Loss 80.793693\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "base_dataset = PDETensorTextDataset(\n",
    "    \"/Users/divyam/Course/Project Arbeit/pde_solver/vl_dataset/annotations.jsonl\"\n",
    ")\n",
    "\n",
    "vision_dataset = VisionOnlyPDEDataset(base_dataset, input_steps=5, output_steps=10)\n",
    "\n",
    "vision_loader = DataLoader(vision_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "vision_model = CNNPDEBaseline(input_steps=5, output_steps=10).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(vision_model.parameters(), lr=3e-4)\n",
    "\n",
    "train_pde_baseline(\n",
    "    vision_model,\n",
    "    vision_loader,\n",
    "    optimizer,\n",
    "    device=device,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6964d0a",
   "metadata": {},
   "source": [
    "\n",
    "# Vision Text CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f0d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDEEncoder(nn.Module):\n",
    "    def __init__(self, input_steps, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_steps, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(128, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, input_steps, X]\n",
    "        z = self.encoder(x).squeeze(-1)  # [B, 128]\n",
    "        return F.normalize(self.proj(z), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbb5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class PDETextCLIP(nn.Module):\n",
    "    def __init__(self, input_steps, text_dim, embed_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vision_encoder = PDEEncoder(input_steps, embed_dim)\n",
    "        self.text_proj = nn.Linear(text_dim, embed_dim)\n",
    "\n",
    "    def forward(self, pde_init, text_emb):\n",
    "        img_emb = self.vision_encoder(pde_init)\n",
    "        txt_emb = F.normalize(self.text_proj(text_emb), dim=-1)\n",
    "        return img_emb, txt_emb\n",
    "    \n",
    "\n",
    "def clip_loss(img_emb, txt_emb, temperature=0.07):\n",
    "    logits = img_emb @ txt_emb.T / temperature\n",
    "    labels = torch.arange(len(img_emb)).to(img_emb.device)\n",
    "\n",
    "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb6a1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDECLIPDataset(Dataset):\n",
    "    def __init__(self, base_dataset, input_steps=5, output_steps=10):\n",
    "        self.base = base_dataset\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        solution, text = self.base[idx]\n",
    "\n",
    "        if isinstance(solution, (tuple, list)):\n",
    "            solution = solution[0]\n",
    "\n",
    "        x = solution[:self.input_steps]   # [5, X]\n",
    "        y = solution[self.input_steps:self.input_steps+10]\n",
    "\n",
    "        return x.float(), y.float(), text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf02270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip(\n",
    "    model,\n",
    "    dataloader,\n",
    "    text_encoder,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=20\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, y, texts in dataloader:\n",
    "            x = x.to(device)  # [B, input_steps, X]\n",
    "            y = y.to(device)  # [B, output_steps, X]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_emb = torch.from_numpy(\n",
    "                    text_encoder.encode(texts)\n",
    "                ).to(device)\n",
    "\n",
    "            img_emb, txt_emb = model(x, text_emb)\n",
    "            loss = clip_loss(img_emb, txt_emb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | CLIP Loss {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ab90a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | CLIP Loss 3.4406\n",
      "Epoch 2 | CLIP Loss 3.4344\n",
      "Epoch 3 | CLIP Loss 3.4304\n",
      "Epoch 4 | CLIP Loss 3.4239\n",
      "Epoch 5 | CLIP Loss 3.4270\n",
      "Epoch 6 | CLIP Loss 3.4129\n",
      "Epoch 7 | CLIP Loss 3.4081\n",
      "Epoch 8 | CLIP Loss 3.4083\n",
      "Epoch 9 | CLIP Loss 3.4007\n",
      "Epoch 10 | CLIP Loss 3.3977\n",
      "Epoch 11 | CLIP Loss 3.3926\n",
      "Epoch 12 | CLIP Loss 3.3824\n",
      "Epoch 13 | CLIP Loss 3.3698\n",
      "Epoch 14 | CLIP Loss 3.3682\n",
      "Epoch 15 | CLIP Loss 3.3606\n",
      "Epoch 16 | CLIP Loss 3.3548\n",
      "Epoch 17 | CLIP Loss 3.3381\n",
      "Epoch 18 | CLIP Loss 3.3312\n",
      "Epoch 19 | CLIP Loss 3.3114\n",
      "Epoch 20 | CLIP Loss 3.2972\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "text_encoder = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "base_dataset = PDETensorTextDataset(\n",
    "    \"/Users/divyam/Course/Project Arbeit/pde_solver/vl_dataset/annotations.jsonl\"\n",
    ")\n",
    "\n",
    "clip_dataset = PDECLIPDataset(\n",
    "    base_dataset,\n",
    "    input_steps=5,\n",
    "    output_steps=10\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    clip_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "vision_text_solver_model = PDETextCLIP(\n",
    "    input_steps=5,\n",
    "    text_dim=384,\n",
    "    embed_dim=512\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    vision_text_solver_model.parameters(),\n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "train_clip(\n",
    "    vision_text_solver_model,\n",
    "    loader,\n",
    "    text_encoder,\n",
    "    optimizer,\n",
    "    device=device,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb13cc",
   "metadata": {},
   "source": [
    "# Vision + Shuffled text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9980da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ShuffledTextPDECLIPDataset(Dataset):\n",
    "    def __init__(self, base_dataset, input_steps=5, output_steps=10):\n",
    "        self.base = base_dataset\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.texts = [s[\"text\"] for s in base_dataset.samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        solution, _ = self.base[idx]\n",
    "\n",
    "        # correct PDE initial condition\n",
    "        x = solution[:self.input_steps].float()  # [5, X]\n",
    "        y = solution[self.input_steps:self.input_steps+10].float()  # [10, X]\n",
    "\n",
    "        # shuffled (incorrect) text\n",
    "        random_text = random.choice(self.texts)\n",
    "\n",
    "        return x, y, random_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fdb0668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | CLIP Loss 3.4686\n",
      "Epoch 2 | CLIP Loss 3.4412\n",
      "Epoch 3 | CLIP Loss 3.4375\n",
      "Epoch 4 | CLIP Loss 3.4373\n",
      "Epoch 5 | CLIP Loss 3.4340\n",
      "Epoch 6 | CLIP Loss 3.4375\n",
      "Epoch 7 | CLIP Loss 3.4384\n",
      "Epoch 8 | CLIP Loss 3.4351\n",
      "Epoch 9 | CLIP Loss 3.4357\n",
      "Epoch 10 | CLIP Loss 3.4360\n",
      "Epoch 11 | CLIP Loss 3.4353\n",
      "Epoch 12 | CLIP Loss 3.4366\n",
      "Epoch 13 | CLIP Loss 3.4353\n",
      "Epoch 14 | CLIP Loss 3.4346\n",
      "Epoch 15 | CLIP Loss 3.4342\n",
      "Epoch 16 | CLIP Loss 3.4354\n",
      "Epoch 17 | CLIP Loss 3.4345\n",
      "Epoch 18 | CLIP Loss 3.4347\n",
      "Epoch 19 | CLIP Loss 3.4358\n",
      "Epoch 20 | CLIP Loss 3.4343\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "shuffled_dataset = ShuffledTextPDECLIPDataset(\n",
    "    base_dataset,\n",
    "    input_steps=5,\n",
    "    output_steps=10\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    shuffled_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_clip(\n",
    "    vision_text_solver_model,\n",
    "    loader,\n",
    "    text_encoder,\n",
    "    optimizer,\n",
    "    device=\"mps\",\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb98a0",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4790c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_base_dataset = PDETensorTextDataset(\n",
    "    \"/Users/divyam/Course/Project Arbeit/pde_solver/vl_dataset/annotations_test.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bec9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vision_dataset = VisionOnlyPDEDataset(\n",
    "    test_base_dataset,\n",
    "    input_steps=5,\n",
    "    output_steps=10\n",
    ")\n",
    "\n",
    "test_vision_loader = DataLoader(\n",
    "    test_vision_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a804c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clip_dataset = PDECLIPDataset(\n",
    "    test_base_dataset,\n",
    "    input_steps=5\n",
    ")\n",
    "\n",
    "test_clip_loader = DataLoader(\n",
    "    test_clip_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False   # CRITICAL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82240a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "test_shuffled_clip_dataset = ShuffledTextPDECLIPDataset(\n",
    "    test_base_dataset,\n",
    "    input_steps=5\n",
    ")\n",
    "\n",
    "test_shuffled_clip_loader = DataLoader(\n",
    "    test_shuffled_clip_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4960cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTextPDESolver(nn.Module):\n",
    "    def __init__(self, vision_solver, text_dim, input_steps):\n",
    "        super().__init__()\n",
    "        self.vision_solver = vision_solver\n",
    "        self.text_proj = nn.Linear(text_dim, input_steps)\n",
    "\n",
    "    def forward(self, x, text_emb):\n",
    "        \"\"\"\n",
    "        x: [B, input_steps, X]  OR  [B, input_steps, 1, X]\n",
    "        text_emb: [B, text_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(2)   # [B, input_steps, 1, X]\n",
    "\n",
    "        # text conditioning\n",
    "        cond = self.text_proj(text_emb)          # [B, input_steps]\n",
    "        cond = cond.unsqueeze(-1).unsqueeze(-1)  # [B, input_steps, 1, 1]\n",
    "\n",
    "        x_cond = x + cond                        # broadcast over X\n",
    "\n",
    "        preds = self.vision_solver(x_cond)       # [B, output_steps, 1, X]\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd26e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_pde_solver(model, dataloader, device):\n",
    "    model.eval()\n",
    "    mse = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        preds = model(x)\n",
    "        mse += F.mse_loss(preds, y, reduction=\"sum\").item()\n",
    "        n += y.numel()\n",
    "\n",
    "    return mse / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_vision_text_solver(\n",
    "    model,\n",
    "    dataloader,\n",
    "    text_encoder,\n",
    "    device\n",
    "):\n",
    "    model.eval()\n",
    "    mse = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y, texts in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        if y.dim() == 3:\n",
    "            y = y.unsqueeze(2) \n",
    "\n",
    "        text_emb = torch.from_numpy(\n",
    "            text_encoder.encode(list(texts))\n",
    "        ).to(device)\n",
    "\n",
    "        preds = model(x, text_emb)\n",
    "\n",
    "        mse += F.mse_loss(preds, y, reduction=\"sum\").item()\n",
    "        n += y.numel()\n",
    "\n",
    "    return mse / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bdfd41c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision-only OOD MSE: 81.26010582829116\n"
     ]
    }
   ],
   "source": [
    "test_mse = evaluate_pde_solver(\n",
    "    vision_model,\n",
    "    test_vision_loader,\n",
    "    device\n",
    ")\n",
    "print(\"Vision-only OOD MSE:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81647201",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vt_dataset = PDECLIPDataset(\n",
    "    test_base_dataset,\n",
    "    input_steps=5,\n",
    "    output_steps=10\n",
    ")\n",
    "\n",
    "test_vt_loader = DataLoader(\n",
    "    test_vt_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7402b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_text_solver_model = VisionTextPDESolver(\n",
    "    vision_solver=vision_model,   \n",
    "    text_dim=384,\n",
    "    input_steps=5\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6b35a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision + text OOD MSE: 81.27684211129446\n"
     ]
    }
   ],
   "source": [
    "test_mse = evaluate_vision_text_solver(\n",
    "    vision_text_solver_model,\n",
    "    test_vt_loader,\n",
    "    text_encoder,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"Vision + text OOD MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c9f87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_shuffled_dataset = ShuffledTextPDECLIPDataset(\n",
    "    test_base_dataset,      # annotations_test.jsonl\n",
    "    input_steps=5,\n",
    "    output_steps=10\n",
    ")\n",
    "\n",
    "test_shuffled_loader = DataLoader(\n",
    "    test_shuffled_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4197e547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision + shuffled text OOD MSE: 81.25692969036487\n"
     ]
    }
   ],
   "source": [
    "shuffled_mse = evaluate_vision_text_solver(\n",
    "    vision_text_solver_model,\n",
    "    test_shuffled_loader,\n",
    "    text_encoder,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"Vision + shuffled text OOD MSE:\", shuffled_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251ee20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
